{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks and Deep Learning (ECBM4040)\n",
    "# Home Work #1\n",
    "----\n",
    "\n",
    "**\n",
    "Author: Yogesh Garg\n",
    "**\n",
    "\n",
    "**\n",
    "Email: yg2482@columbia.edu\n",
    "**\n",
    "\n",
    "**\n",
    "Collaborators:\n",
    "**\n",
    " * Richard Godden (rg3047)\n",
    " * Jonathan Chan (jc4659)\n",
    "-----\n",
    "## Part A\n",
    "\n",
    "    python hw1a.py\n",
    "-----\n",
    "## Part B\n",
    "\n",
    "    python hw1b.py\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudocode for the **standard gradient descent** for finding the eigen value of a matrix X is as follows:\n",
    "\n",
    "\n",
    "    for i = 0, ... , N do:\n",
    "        A_i <- X.T * X - \\sum_{j=0}_{i-1} \\lambda_j d_j d_j.T\n",
    "        initialise d_i randomly and let t = 1\n",
    "        while( t <= T & stop_condition == False ) do:\n",
    "            y <- d_i - \\eta \\delta_{d_i}(-d_i.T * A_i * d_i)\n",
    "            d_i <- \\frac{y}{\\| y \\|}\n",
    "            t <- t+1\n",
    "        \\lambda_i <- d_i.T * X.T * X * d_i\n",
    "\n",
    "In this formulation, we are trying to maximize our objective function A_i (which is of size 65536 x 65536) in each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) here, the trick lies in breaking the objective function into summable components:\n",
    "\n",
    "$Q(w) = \\frac{1}{n} \\sum_{i=1}^{n} Q_i(w)$\n",
    "\n",
    "We can note that if we consider just one image at a time\n",
    "\n",
    "Each image is given by the following:\n",
    "$$\n",
    "X_i = ( x_{i,1}, x_{i,2}, x_{i,3} ... x_{i,65536}) \\\\\n",
    "$$\n",
    "The whole matrix is given by\n",
    "$$\n",
    "X = (X_1, X_2, ... , X_{200}) \\\\\n",
    "$$\n",
    "Notice that $X^T X$ is simply a summation accross all images:\n",
    "$$\n",
    "(X^T X)_{m,n} = \\sum_{i=1}^{200} X_{i,m} X_{i,n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can modify standard gradient descent to consider a single image at a time.\n",
    "\n",
    "This gives us the following pseudocode for the stochastic gradient descent for finding the eigen value of a matrix X is as follows:\n",
    "\n",
    "    for i = 0, ... , N do:\n",
    "        (A_i)_k <- X_k.T * X_k - \\sum_{j=0}_{i-1} \\lambda_j d_j d_j.T \\forall k\n",
    "        initialise d_i randomly and let t = 1\n",
    "        while( t <= T & stop_condition == False ) do:\n",
    "            k <- choose an image randomly\n",
    "            y <- d_i - \\eta \\delta_{d_i}(-d_i.T * (A_i)_k * d_i)\n",
    "            d_i <- \\frac{y}{\\| y \\|}\n",
    "            t <- t+1\n",
    "        \\lambda_i <- d_i.T * X.T * X * d_i\n",
    "\n",
    "-----\n",
    "\n",
    "<!-- PLEASE IGNORE THIS\n",
    "    for i = 0, ... , N do:\n",
    "        A_i <- X.T * X - \\sum_{j=0}_{i-1} \\lambda_j d_j d_j.T\n",
    "        initialise d_i randomly and let t = 1\n",
    "        while( t <= T & stop_condition == False ) do:\n",
    "            r <- choose a random dimension \n",
    "            y <- d_i - \\eta \\delta_{d_i}(- d_{i,r}.T * {A_i}_{r,r} * d_{i,r})\n",
    "            d_i <- \\frac{y}{\\| y \\|}\n",
    "            t <- t+1\n",
    "        \\lambda_i <- d_i.T * X.T * X * d_i\n",
    "PLEASE IGNORE THIS -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D\n",
    "### 1. Histogram Equalization of Softmax\n",
    "<img src=\"imgs/d11.jpg\" width=75%>\n",
    "<img src=\"imgs/d12.jpg\" width=75%>\n",
    "<img src=\"imgs/d13.jpg\" width=75%>\n",
    "The given function is called the **softmax function**, $\\Phi$ is the integral of standard normal distribution and the function *erf(x)* is called the **error function**\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multivariate Probability Distribution Functions\n",
    "\n",
    "<img src=\"imgs/d21.jpg\" width=75%>\n",
    "<img src=\"imgs/d22.jpg\" width=75%>\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part E\n",
    "\n",
    "### 1.\n",
    "<img src='imgs/e11.jpg' width=75%>\n",
    "<img src='imgs/e12.jpg' width=75%>\n",
    "<img src='imgs/e13.jpg' width=75%>\n",
    "\n",
    "### 2.\n",
    "\n",
    "Because mean is a linear combination of sample mean and mean_0, it is **un biased**\n",
    "\n",
    "Variance is **biased** because it has terms of m inside it which are inverted.\n",
    "\n",
    "### 3.\n",
    "\n",
    "MLE for sigma doesn't depend on the second term, so it turns out to be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
